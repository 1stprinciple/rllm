from abc import ABC, abstractmethod

class BaseAgent(ABC):
    @abstractmethod
    def _pre_get_action(self, trajectory):
        """
        Prepares the input for model query by formatting the existing trajectory into the ChatML format.

        Args:
            trajectory (List[Dict]): The current trajectory containing observations and actions. The newest observation is at the end.
                Each step is structures as a dictionary with key 'observation' 'next_observation' 'reward' 'action' 'response'
                The first element is sentinel and only have 'next_observation'.
                Newest observation is 'next_observation' of the last step.

        Returns:
            List[Dict[str, str]]: A list of dictionaries formatted in ChatML, where each dictionary
            contains a "role" (e.g., "system", "user") and "content" (the corresponding text).
        """
        return [
            {"role": "system", "content": ""},
            {"role": "user", "content": ""},
        ]

    @abstractmethod
    def _post_get_action(self, response):
        """
        Processes the response generated by `get_action` and extracts the final action.

        Args:
            response (str): The raw response generated by the model.

        Returns:
            str: The extracted action as a string.
        """
        return response

    @abstractmethod
    def update(self, action, observation, next_observation, reward, terminated, truncated, info):
        """
        Updates the agent's internal state after an environment step.

        This function is called during environment interaction to incorporate the latest action's
        outcome into the agent's learning process.

        Args:
            action (str): The action taken by the agent.
            observation (Any): The observation before taking the action.
            next_observation (Any): The observation after taking the action.
            reward (float): The reward received after taking the action.
            terminated (bool): Whether the episode has ended due to termination.
            truncated (bool): Whether the episode has ended due to truncation.
            info (dict): Additional metadata from the environment.

        Returns:
            None
        """
        return

    @abstractmethod
    def reset(self):
        """
        Resets the agent's internal state, typically called at the beginning of a new episode.

        This function should clear any stored history or state information necessary 
        for a fresh interaction.

        Returns:
            None
        """
        return
    
    @abstractmethod
    def compute_training_reward(self, trajectory):
        """
        Adjusts or augments the reward signal based on the entire trajectory to use for RL training.

        Args:
            trajectory (Any): The complete trajectory.

        Returns:
            float: The modified reward value.
        """
        return trajectory[-1]["reward"]
    
    @abstractmethod
    def convert_observation_to_string(self, obs, with_system_prompt=False):
        """
        Convert an observation into a formatted string representation.

        Args:
            obs (Any): The observation to be converted. The exact structure depends on the environment.
            with_system_prompt (bool, optional): If True, includes a system prompt in the formatted output.
                                                 Defaults to False.

        Returns:
            str: The formatted string representation of the observation.
        """
        return ""