"""Data types for sampling from language models.

This module defines the core data structures used to represent model outputs and batches
of samples during inference.
"""
from dataclasses import dataclass
from typing import Dict, List, Union

@dataclass(slots=True, frozen=True)
class SampleConfig:
    """Configuration for language model sampling.

    Controls parameters for distributed sampling from language models, including batch sizes,
    parallelization settings, and sampling temperature.

    Attributes:
        samples_per_problem: Number of samples to generate for each input problem/prompt.
        num_workers: Number of parallel sampling workers to distribute inference across.
        tensor_parallel_size: Degree of tensor parallelism for each worker.
        sampler_backend: Backend framework for sampling ("SGLang" or "VLLM").
        model: Path or name of the language model to use for sampling.
        temperature: Sampling temperature - higher values increase randomness (0.0-2.0).
        max_tokens: Maximum number of tokens to generate for each sample.
    """
    samples_per_problem: int = 4
    num_workers: int = 1
    tensor_parallel_size: int = 2
    sampler_backend: str = "SGLang"
    model: str = "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"
    temperature: float = 0.6
    max_tokens: int = 8096

@dataclass(slots=True, kw_only=True)
class Sample:
    """A single sample generated by the language model.

    Represents one model-generated response along with associated metadata like
    probabilities and evaluation metrics.

    Attributes:
        response: Raw text response from the model
        tokens: Tokenized form of the response as integer IDs
        mask: Binary mask for loss computation (True=include, False=exclude)
        log_probs: Log probabilities for each generated token
        reward: Scalar quality metric for the response
        is_correct: Whether the response meets correctness criteria
    """
    response: str
    tokens: List[int]
    mask: List[bool]
    log_probs: List[float]
    reward: float
    is_correct: bool


@dataclass(slots=True, kw_only=True)
class SampleBatch:
    """A batch of model-generated samples sharing the same prompt.

    Groups multiple samples together with their shared context and aggregate metrics.

    Attributes:
        prompt: Input prompt text used to generate the samples
        prompt_tokens: Tokenized form of the prompt as integer IDs
        samples: List of generated Sample objects
        metrics: Aggregate statistics over the batch (e.g. mean reward)
    """
    prompt: str
    prompt_tokens: List[int]
    samples: List[Sample]
    metrics: Dict[str, Union[float, int]]