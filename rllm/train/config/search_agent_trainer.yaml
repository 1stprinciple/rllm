# Search Agent Training Configuration
# Inherits from ppo_trainer.yaml and overrides specific values

defaults:
  - ppo_trainer
  - _self_

# Data configuration for search tasks
data:
  train_files: null  # Will be set by dataset registry
  val_files: null    # Will be set by dataset registry
  max_prompt_length: 1024
  max_response_length: 512
  train_batch_size: 32

# Model configuration - Use Qwen3-4B for all components
actor_rollout_ref:
  model:
    path: Qwen/Qwen3-4B
    tokenizer_path: Qwen/Qwen3-4B
  actor:
    ppo_mini_batch_size: 32
    ppo_micro_batch_size_per_gpu: 1
    async_engine: False
  ref:
    log_prob_micro_batch_size_per_gpu: 1
  rollout:
    enable_log_prob: false
    temperature: 0.7
    top_p: 0.9
    top_k: 50
    response_length: 512
    tensor_model_parallel_size: 1
    log_prob_micro_batch_size_per_gpu: 1
    val_kwargs:
      temperature: 0.7
      top_p: 0.95
      do_sample: True

critic:
  model:
    path: Qwen/Qwen3-4B
    tokenizer_path: Qwen/Qwen3-4B
  ppo_micro_batch_size_per_gpu: 1

reward_model:
  model:
    input_tokenizer: Qwen/Qwen2.5-3B-Instruct
  micro_batch_size_per_gpu: 1

# Trainer configuration for smaller scale training
trainer:
  n_gpus_per_node: 1
  val_before_train: false
  project_name: rllm_search
  experiment_name: search_agent

# Environment configuration for tool usage
env:
  name: tool
  env_args:
    max_steps: 3
    tools: ["local_search"]
    retrieval_server_url: ${oc.env:RETRIEVAL_SERVER_URL,http://127.0.0.1:9000}

# Agent configuration for tool agent
agent:
  name: tool_agent
  max_steps: 5
  async_engine: false
  agent_args: {} 